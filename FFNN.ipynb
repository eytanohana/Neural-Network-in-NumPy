{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network\n",
    "\n",
    "By the end of the notebook we are going to create a simple feed-forward neural net that learns to recognize handwritten digits using the [MNIST-dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "We'll first start by training a simple neural network to learn to classify XOR ($\\oplus$):\n",
    "\n",
    "<table>\n",
    "    <thead><tr><td>a</td><td>b</td><td>$a \\oplus b$</td></tr></thead>\n",
    "    <tbody>\n",
    "        <tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "        <tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "We'll start by defining the structure of our network:\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/2-3-1-nn.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "- The first layer (aka the input layer) has two inputs corresponding to $a$ and $b$.\n",
    "- The middle / hidden layer is composed of three neurons.\n",
    "- The final layer (aka the output layer) has a single output. \n",
    "\n",
    "The output of the neural network is a prediction that is the probability of the instance to belong to the positive class.\n",
    "So anything above 0.5 we could classify as 1 and everthing below 0.5 we could classify as 0.\n",
    "\n",
    "We start by setting up our data according to the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "y:\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([0,1,1,0])\n",
    "\n",
    "print('X:')\n",
    "print(X, '\\n')\n",
    "print('y:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the layers\n",
    "\n",
    "It's common practice to initialize the weights of each layer by drawing from a uniform distribution ranging from \n",
    "$$\n",
    "-\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}} \\to \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "\n",
    "also known as the **Glorot uniform**.\n",
    "\n",
    "We'll creat the network above meaning we have two weight matrices with shapes 2x3 and 3x1. The output of the first layer will go through a relu activation while the output of the second layer will go through a sigmoid activation. The result will be the output of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13806544  0.60864744  0.29065872]\n",
      " [ 0.12694881 -0.21593684  0.41265087]]\n",
      "\n",
      "[[-0.17653002]\n",
      " [ 1.10810138]\n",
      " [ 1.31143633]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_inputs, hidden_size, n_outputs = 2, 3, 1\n",
    "\n",
    "bounds = np.sqrt(6 / (n_inputs + n_outputs))\n",
    "\n",
    "weights1 = np.random.uniform(-bounds, bounds, (n_inputs, hidden_size))\n",
    "weights2 = np.random.uniform(-bounds, bounds, (hidden_size, n_outputs))\n",
    "\n",
    "print(weights1)\n",
    "print()\n",
    "print(weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.12694881, -0.21593684,  0.41265087],\n",
       "       [ 0.13806544,  0.60864744,  0.29065872],\n",
       "       [ 0.26501425,  0.3927106 ,  0.70330959]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.12694881 -0.21593684  0.41265087]\n",
      " [ 0.13806544  0.60864744  0.29065872]\n",
      " [ 0.26501425  0.3927106   0.70330959]]\n",
      "\n",
      "X2\n",
      "[[0.         0.         0.        ]\n",
      " [0.12694881 0.         0.41265087]\n",
      " [0.13806544 0.60864744 0.29065872]\n",
      " [0.26501425 0.3927106  0.70330959]]\n",
      "\n",
      "Z2\n",
      "[[0.        ]\n",
      " [0.51875506]\n",
      " [1.03125078]\n",
      " [1.31072593]]\n",
      "\n",
      "Yhat\n",
      "[[0.5       ]\n",
      " [0.62685661]\n",
      " [0.73715831]\n",
      " [0.78763461]]\n"
     ]
    }
   ],
   "source": [
    "# forwarding through the network.\n",
    "z1 = X @ weights1\n",
    "x2 = np.maximum(z1, 0)\n",
    "z2 = x2 @ weights2\n",
    "y_hat = 1 / (1 + np.exp(-z2))\n",
    "\n",
    "print('Z1')\n",
    "print(z1)\n",
    "print('\\nX2')\n",
    "print(x2)\n",
    "print('\\nZ2')\n",
    "print(z2)\n",
    "print('\\nYhat')\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to interpret $Z_1$. Our input was a batch of 4 instances with 2 \"features\" each. Our hidden layer has 3 neurons corresponding to the 3 columns in $Z_1$. Each input instance corresponds to each row in $Z_1$. \n",
    "\n",
    "Meaning the number in the 3rd column and second row of $Z_1$ is the output of the 3rd neuron in the hidden layer for the second input instance (in this case \\[0, 1\\]).\n",
    "\n",
    "The same logic follows for $X_2$, $Z_2$, and $\\hat Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fully connected layer: (2, 3)\n",
      "2) Fully connected layer: (3, 1)\n",
      "\n",
      "Forwarding X:\n",
      "\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "The models predictions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = nn.NN([\n",
    "    nn.Layer(2, 3, activation=nn.ReLU),\n",
    "    nn.Layer(3, 1, activation=nn)\n",
    "])\n",
    "\n",
    "print(model)\n",
    "print('Forwarding X:\\n')\n",
    "print(model.forward(X))\n",
    "\n",
    "print('\\nThe models predictions:')\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the network is predicting everything to be a 0. \n",
    "\n",
    "So we need to learn right weights to give the correct output. This is where backpropagation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In backpropagation we learn what the right set of weights are in order to give the desired output.\n",
    "\n",
    "In this example assume our network is a general 3 layer network like below (instead of 2). This will help illustrate the pattern that arises for backpropagation. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/3-layer-nn.png\" width=\"60%\" />\n",
    "</div>\n",
    "\n",
    "We assume we have a cost function (denoted $J$, in this case cross-entropy loss). We find the partial derivatives of the cost function with respect to each weight matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3} =  \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{W}_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{X}_2} \\cdot \\frac{\\partial \\mathbf{X}_2}{\\partial \\mathbf{Z}_1} \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's break down the formulas:**\n",
    "\n",
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3}$:\n",
    "\n",
    "$\\text{We'll set }\\delta_3 = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} = \\mathbf{\\hat{Y}} - \\mathbf{Y}$\n",
    "\n",
    "Since $\\mathbf{Z}_3 = \\mathbf{X}_3 \\cdot \\mathbf{W}_3 \\to \\frac{\\partial \\mathbf{Z}_3}{{\\partial \\mathbf{W}_3}} = \\mathbf{X}_3$ So in total:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3} = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3} = {\\mathbf{X}_3}^T \\cdot \\delta_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2}$:\n",
    "\n",
    "\n",
    "$\\text{Now set }\\delta_2 = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2}$\n",
    "\n",
    "Since $\\mathbf{Z}_3 = \\mathbf{X}_3 \\cdot \\mathbf{W}_3 \\to \\frac{\\partial \\mathbf{Z}_3}{{\\partial \\mathbf{X}_3}} = \\mathbf{W}_3$ and $\\mathbf{X}_3 = \\text{ReLU}(\\mathbf{Z}_2) \\text{ so } \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} = \\text{ReLU}'(\\mathbf{Z}_2)$ therefore:\n",
    "\n",
    "$$\n",
    "\\delta_2 = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} = \\delta_3 \\cdot {\\mathbf{W}_3}^T * \\text{ReLU}'(\\mathbf{Z}_2)\n",
    "$$\n",
    "*Note: * indicates element-wise multiplication.*\n",
    "\n",
    "Now notice:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2} = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z}_2}{{\\partial \\mathbf{W}_2}} = {\\mathbf{X}_2}^T \\cdot \\delta_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1}$:\n",
    "\n",
    "\n",
    "$\\text{Now set }\\delta_1 = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{X}_2} \\cdot \\frac{\\partial \\mathbf{X}_2}{\\partial \\mathbf{Z}_1} \\to \\delta_1 = \\delta_2 \\cdot {\\mathbf{W}_2}^T * \\text{ReLU}'(\\mathbf{Z}_1)$\n",
    "\n",
    "and notice:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1} = \\delta_1 \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1} = {\\mathbf{X}_1}^T \\cdot \\delta_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\mathbf{Z}_3) = 1 + e^{- \\mathbf{Z}_3}' = \\frac{0 \\cdot (1 + e^{-\\mathbf{Z}_3}) - (1 + e^{-\\mathbf{Z}_3})(-\\mathbf{Z}_3)}{(1 + e^{-\\mathbf{Z}_3})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science 3.7",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
