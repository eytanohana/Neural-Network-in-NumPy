{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "\n",
    "Note: This notebook already assumes a basic knowledge of neural nets. Things like layers and layer sizes, activation functions, batching, softmax, and so on.\n",
    "\n",
    "By the end of the notebook we are going to create a simple feed-forward neural net that learns to recognize handwritten digits using the [MNIST-dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "We'll first start by training a simple neural network to learn to classify XOR:\n",
    "\n",
    "<table>\n",
    "    <thead><tr><td>a</td><td>b</td><td>a XOR b</td></tr></thead>\n",
    "    <tbody>\n",
    "        <tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "        <tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "We'll start by defining the structure of our network:\n",
    "<img src=\"XOR-nn.png\" width=\"60%\">\n",
    "\n",
    "- The first layer (aka the input layer) has two inputs corresponding to $a$ and $b$.\n",
    "- The middle / hidden layer is composed of three neurons.\n",
    "- The final layer (aka the output layer) has two outputs. \n",
    "\n",
    "The output of the neural network is a vector of length 2 where the first entry is the probability of the result being 0 and the second entry is the probability of the result being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward\n",
    "\n",
    "It's called a **Feed-Forward Neural Net** because we **feed the input forward** through the network starting at the input layer until the output.\n",
    "\n",
    "Here's how we implement the feed forward algorithm.\n",
    "\n",
    "$$\n",
    "Z_1 = X_1 \\cdot W_1 \\\\\n",
    "X_2 = \\text{ReLU}(Z_1) \\\\\n",
    "Z_2 = X_2 \\cdot W_2 \\\\\n",
    "\\hat{Y} = \\text{Softmax}(Z_2)\n",
    "$$\n",
    "\n",
    "Note: $\\large \\cdot$ represents matrix multiplication.\n",
    "\n",
    "To start we'll get some notation out of the way:\n",
    "1. **X1** is the input. \n",
    "    - It can either be a single instance i.e. \\[0, 0\\] (1 x 2) or a batch of instances \\[[0,0],[0,1],[1,0]] (3 x 2)\n",
    "1. **W1** is the first weight matrix with a shape of (2 x 3)\n",
    "2. **W2** is the second weight matrix with a shape of (3 x 2)\n",
    "\n",
    "- We forward our input through the first layer and get out $Z_1$. \n",
    "- We then apply a ReLU activation function on $Z_1$ and get $X_2$. \n",
    "- We then forward $X_2$ through the second layer and get $Z_2$.\n",
    "- Finally we apply softmax on $Z_2$ to get a vector of probabilities, $\\hat{Y}$, for each class (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp = np.exp(X - X.max(axis=1, keepdims=True))\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def ReLU(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def forward(Ws, X):\n",
    "    X = np.atleast_2d(X)\n",
    "    W1, W2 = Ws\n",
    "    \n",
    "    Z1 = X @ W1\n",
    "    X2 = ReLU(Z1)\n",
    "    Z2 = X2 @ W2\n",
    "    Yhat = softmax(Z2)\n",
    "    return [X, Z1, X2, Yhat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a list with the intermediate values to be used later in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# y is \"one-hotted\": Since the first row\n",
    "# has a 1 in the zeroth position, it means\n",
    "# that the output is a 0.\n",
    "# Since in the second row the 1 is in the 1st\n",
    "# position, it means the output is a 1.\n",
    "y = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
    "\n",
    "print(X, '\\n')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the weight matrices\n",
    "\n",
    "It's common practice to initialize the weights by drawing from a uniform distribution from \n",
    "$$\n",
    "-\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}} \\to \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "\n",
    "also known as **Glorot uniform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(ninputs, noutputs):\n",
    "    boundary = np.sqrt(6 / (ninputs + noutputs))\n",
    "    return np.random.uniform(-boundary, boundary, size=(ninputs, noutputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[ 0.10694503  0.47145628  0.22514328]\n",
      " [ 0.09833413 -0.16726395  0.31963799]] \n",
      "\n",
      "W2\n",
      "[[-0.13673957  0.85833164]\n",
      " [ 1.01583421 -0.25536684]\n",
      " [ 0.63913754  0.0633056 ]] \n",
      "\n",
      "Preliminary predictions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "W1 = init_weights(2, 3)\n",
    "W2 = init_weights(3, 2)\n",
    "Ws = [W1, W2]\n",
    "\n",
    "print('W1')\n",
    "print(W1, '\\n')\n",
    "print('W2')\n",
    "print(W2, '\\n')\n",
    "\n",
    "print('Preliminary predictions:')\n",
    "forward(Ws, X)[-1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the network is predicting everything to be a 0. \n",
    "\n",
    "So we need to learn right weights to give the correct output. This is where backpropagation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In backpropagation we learn what the right set of weights are in order to give the desired output.\n",
    "\n",
    "We assume we have a cost function (denoted $J$, in this case cross-entropy loss). We find the partial derivatives of the cost function with respect to each weight matrix. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_2} = \\frac{\\partial J}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial W_2} \\\\\n",
    "\\frac{\\partial J}{\\partial W_1} = \\frac{\\partial J}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial X_2} \\cdot \\frac{\\partial X_2}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
