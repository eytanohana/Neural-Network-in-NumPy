{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "y:\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "\n",
      "y is \"one-hotted\". \n",
      "\n",
      "Each row in X corresponds to an output row in y. \n",
      "\n",
      "Since the first row of y has a 1 in \n",
      "the zeroth position, it means that \n",
      "the output is a 0.\n",
      "\n",
      "Since in the second row the 1 is in the 1st\n",
      "position, it means the output is a 1.\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
    "\n",
    "print('X:')\n",
    "print(X, '\\n')\n",
    "print('y:')\n",
    "print(y)\n",
    "\n",
    "print('''\n",
    "y is \"one-hotted\". \n",
    "\n",
    "Each row in X corresponds to an output row in y. \n",
    "\n",
    "Since the first row of y has a 1 in \n",
    "the zeroth position, it means that \n",
    "the output is a 0.\n",
    "\n",
    "Since in the second row the 1 is in the 1st\n",
    "position, it means the output is a 1.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the layers\n",
    "\n",
    "It's common practice to initialize the weights of each layer by drawing from a uniform distribution ranging from \n",
    "$$\n",
    "-\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}} \\to \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "\n",
    "also known as the **Glorot uniform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fully connected layer: (2, 3)\n",
      "2) Fully connected layer: (3, 2)\n",
      "\n",
      "Forwarding X:\n",
      "\n",
      "[[0.5        0.5       ]\n",
      " [0.52153874 0.47846126]\n",
      " [0.65079658 0.34920342]\n",
      " [0.62154818 0.37845182]]\n",
      "\n",
      "The models predictions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "model = nn.NN([\n",
    "    nn.Layer(2, 3),\n",
    "    nn.Layer(3, 2)\n",
    "])\n",
    "\n",
    "print(model)\n",
    "print('Forwarding X:\\n')\n",
    "print(model.forward(X))\n",
    "\n",
    "print('\\nThe models predictions:')\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the network is predicting everything to be a 0. \n",
    "\n",
    "So we need to learn right weights to give the correct output. This is where backpropagation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In backpropagation we learn what the right set of weights are in order to give the desired output.\n",
    "\n",
    "In this example assume our network is a general 3 layer network like below (instead of 2). This will help illustrate the pattern that arises for backpropagation. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/3-layer-nn.png\" width=\"60%\" />\n",
    "</div>\n",
    "\n",
    "We assume we have a cost function (denoted $J$, in this case cross-entropy loss). We find the partial derivatives of the cost function with respect to each weight matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3} =  \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{W}_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{X}_2} \\cdot \\frac{\\partial \\mathbf{X}_2}{\\partial \\mathbf{Z}_1} \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's break down the formulas:**\n",
    "\n",
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3}$:\n",
    "\n",
    "$\\text{We'll set }\\delta_3 = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} = \\mathbf{\\hat{Y}} - \\mathbf{Y}$\n",
    "\n",
    "Since $\\mathbf{Z}_3 = \\mathbf{X}_3 \\cdot \\mathbf{W}_3 \\to \\frac{\\partial \\mathbf{Z}_3}{{\\partial \\mathbf{W}_3}} = \\mathbf{X}_3$ So in total:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3} = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3} = {\\mathbf{X}_3}^T \\cdot \\delta_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2}$:\n",
    "\n",
    "\n",
    "$\\text{Now set }\\delta_2 = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2}$\n",
    "\n",
    "Since $\\mathbf{Z}_3 = \\mathbf{X}_3 \\cdot \\mathbf{W}_3 \\to \\frac{\\partial \\mathbf{Z}_3}{{\\partial \\mathbf{X}_3}} = \\mathbf{W}_3$ and $\\mathbf{X}_3 = \\text{ReLU}(\\mathbf{Z}_2) \\text{ so } \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} = \\text{ReLU}'(\\mathbf{Z}_2)$ therefore:\n",
    "\n",
    "$$\n",
    "\\delta_2 = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} = \\delta_3 \\cdot {\\mathbf{W}_3}^T * \\text{ReLU}'(\\mathbf{Z}_2)\n",
    "$$\n",
    "*Note: * indicates element-wise multiplication.*\n",
    "\n",
    "Now notice:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2} = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z}_2}{{\\partial \\mathbf{W}_2}} = {\\mathbf{X}_2}^T \\cdot \\delta_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1}$:\n",
    "\n",
    "\n",
    "$\\text{Now set }\\delta_1 = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{X}_2} \\cdot \\frac{\\partial \\mathbf{X}_2}{\\partial \\mathbf{Z}_1} \\to \\delta_1 = \\delta_2 \\cdot {\\mathbf{W}_2}^T * \\text{ReLU}'(\\mathbf{Z}_1)$\n",
    "\n",
    "and notice:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1} = \\delta_1 \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1} = {\\mathbf{X}_1}^T \\cdot \\delta_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science 3.7",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
