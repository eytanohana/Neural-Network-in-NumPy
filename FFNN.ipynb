{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "\n",
    "Note: This notebook already assumes a basic knowledge of neural nets. Things like layers and layer sizes, activation functions, batching, softmax, and so on.\n",
    "\n",
    "By the end of the notebook we are going to create a simple feed-forward neural net that learns to recognize handwritten digits using the [MNIST-dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "We'll first start by training a simple neural network to learn to classify XOR:\n",
    "\n",
    "<table>\n",
    "    <thead><tr><td>a</td><td>b</td><td>a XOR b</td></tr></thead>\n",
    "    <tbody>\n",
    "        <tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "        <tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "We'll start by defining the structure of our network:\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"XOR-nn.png\" width=\"60%\">\n",
    "</div>\n",
    "- The first layer (aka the input layer) has two inputs corresponding to $a$ and $b$.\n",
    "- The middle / hidden layer is composed of three neurons.\n",
    "- The final layer (aka the output layer) has two outputs. \n",
    "\n",
    "The output of the neural network is a vector of length 2 where the first entry is the probability of the result being 0 and the second entry is the probability of the result being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward\n",
    "\n",
    "It's called a **Feed-Forward Neural Net** because we **feed the input forward** through the network starting at the input layer until the output.\n",
    "\n",
    "Here's how we implement the feed forward algorithm.\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_1 = \\mathbf{X}_1 \\cdot \\mathbf{W}_1 \\\\\n",
    "\\mathbf{X}_2 = \\text{ReLU}(\\mathbf{Z}_1) \\\\\n",
    "\\mathbf{Z}_2 = \\mathbf{X}_2 \\cdot \\mathbf{W}_2 \\\\\n",
    "\\mathbf{\\hat{Y}} = \\text{Softmax}(\\mathbf{Z_2})\n",
    "$$\n",
    "\n",
    "Note: $\\large \\cdot$ represents matrix multiplication.\n",
    "\n",
    "To start we'll get some notation out of the way:\n",
    "1. **X1** is the input. \n",
    "    - It can either be a single instance i.e. \\[0, 0\\] (1 x 2) or a batch of instances \\[[0,0],[0,1],[1,0]] (3 x 2)\n",
    "1. **W1** is the first weight matrix with a shape of (2 x 3)\n",
    "2. **W2** is the second weight matrix with a shape of (3 x 2)\n",
    "\n",
    "- We forward our input through the first layer and get out $Z_1$. \n",
    "- We then apply a ReLU activation function on $Z_1$ and get $X_2$. \n",
    "- We then forward $X_2$ through the second layer and get $Z_2$.\n",
    "- Finally we apply softmax on $Z_2$ to get a vector of probabilities, $\\hat{Y}$, for each class (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "y:\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "\n",
      "y is \"one-hotted\". \n",
      "\n",
      "Each row in X corresponds to an output row in y. \n",
      "\n",
      "Since the first row of y has a 1 in \n",
      "the zeroth position, it means that \n",
      "the output is a 0.\n",
      "\n",
      "Since in the second row the 1 is in the 1st\n",
      "position, it means the output is a 1.\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# y is \"one-hotted\": Since the first row\n",
    "# has a 1 in the zeroth position, it means\n",
    "# that the output is a 0.\n",
    "# Since in the second row the 1 is in the 1st\n",
    "# position, it means the output is a 1.\n",
    "y = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
    "\n",
    "print('X:')\n",
    "print(X, '\\n')\n",
    "print('y:')\n",
    "print(y)\n",
    "\n",
    "print('''\n",
    "y is \"one-hotted\". \n",
    "\n",
    "Each row in X corresponds to an output row in y. \n",
    "\n",
    "Since the first row of y has a 1 in \n",
    "the zeroth position, it means that \n",
    "the output is a 0.\n",
    "\n",
    "Since in the second row the 1 is in the 1st\n",
    "position, it means the output is a 1.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the layers\n",
    "\n",
    "It's common practice to initialize the weights of each layer by drawing from a uniform distribution ranging from \n",
    "$$\n",
    "-\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}} \\to \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "\n",
    "also known as **Glorot uniform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fully connected layer: (2, 3)\n",
      "2) Fully connected layer: (3, 2)\n",
      "\n",
      "Forwarding X:\n",
      "\n",
      "[[0.5        0.5       ]\n",
      " [0.52153874 0.47846126]\n",
      " [0.65079658 0.34920342]\n",
      " [0.62154818 0.37845182]]\n",
      "\n",
      "The models predictions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "model = nn.NN([\n",
    "    nn.Layer(2, 3),\n",
    "    nn.Layer(3, 2)\n",
    "])\n",
    "\n",
    "print(model)\n",
    "print('Forwarding X:\\n')\n",
    "print(model.forward(X))\n",
    "\n",
    "print('\\nThe models predictions:')\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the network is predicting everything to be a 0. \n",
    "\n",
    "So we need to learn right weights to give the correct output. This is where backpropagation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In backpropagation we learn what the right set of weights are in order to give the desired output.\n",
    "\n",
    "In this example assume our network is a general 3 layer network like below (instead of 2). This will help illustrate the pattern that arises for backpropagation. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"3-layer-nn.png\" width=\"60%\" />\n",
    "</div>\n",
    "\n",
    "We assume we have a cost function (denoted $J$, in this case cross-entropy loss). We find the partial derivatives of the cost function with respect to each weight matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3} =  \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{W}_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{X}_2} \\cdot \\frac{\\partial \\mathbf{X}_2}{\\partial \\mathbf{Z}_1} \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's break down the formulas:**\n",
    "\n",
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3}$:\n",
    "\n",
    "$\\text{We'll set }\\delta_3 = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} = \\mathbf{\\hat{Y}} - \\mathbf{Y}$\n",
    "\n",
    "Since $\\mathbf{Z}_3 = \\mathbf{X}_3 \\cdot \\mathbf{W}_3 \\to \\frac{\\partial \\mathbf{Z}_3}{{\\partial \\mathbf{W}_3}} = \\mathbf{X}_3$ So in total:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_3} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\hat{Y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{Y}}}{\\partial \\mathbf{Z}_3} \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{W}_3} = {\\mathbf{X}_3}^T \\cdot \\delta_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2}$:\n",
    "\n",
    "\n",
    "$\\text{Now set }\\delta_2 = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2}$\n",
    "\n",
    "Since $\\mathbf{Z}_3 = \\mathbf{X}_3 \\cdot \\mathbf{W}_3 \\to \\frac{\\partial \\mathbf{Z}_3}{{\\partial \\mathbf{X}_3}} = \\mathbf{W}_3$ and $\\mathbf{X}_3 = \\text{ReLU}(\\mathbf{Z}_2) \\text{ so } \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} = \\text{ReLU}'(\\mathbf{Z}_2)$ therefore:\n",
    "\n",
    "$$\n",
    "\\delta_2 = \\delta_3 \\cdot \\frac{\\partial \\mathbf{Z}_3}{\\partial \\mathbf{X}_3} \\cdot \\frac{\\partial \\mathbf{X}_3}{\\partial \\mathbf{Z}_2} = \\delta_3 \\cdot {\\mathbf{W}_3}^T * \\text{ReLU}'(\\mathbf{Z}_2)\n",
    "$$\n",
    "*Note: * indicates element-wise multiplication.*\n",
    "\n",
    "Now notice:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_2} = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z}_2}{{\\partial \\mathbf{W}_2}} = {\\mathbf{X}_2}^T \\cdot \\delta_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1}$:\n",
    "\n",
    "\n",
    "$\\text{Now set }\\delta_1 = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{X}_2} \\cdot \\frac{\\partial \\mathbf{X}_2}{\\partial \\mathbf{Z}_1} \\to \\delta_1 = \\delta_2 \\cdot {\\mathbf{W}_2}^T * \\text{ReLU}'(\\mathbf{Z}_1)$\n",
    "\n",
    "and notice:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W}_1} = \\delta_1 \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1} = {\\mathbf{X}_1}^T \\cdot \\delta_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
