{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "\n",
    "Note: This notebook already assumes a basic knowledge of neural nets. Things like layers and layer sizes, activation functions, batching, softmax, and so on.\n",
    "\n",
    "By the end of the notebook we are going to create a simple feed-forward neural net that learns to recognize handwritten digits using the [MNIST-dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "We'll first start by training a simple neural network to learn to classify XOR:\n",
    "\n",
    "<table>\n",
    "    <thead><tr><td>a</td><td>b</td><td>a XOR b</td></tr></thead>\n",
    "    <tbody>\n",
    "        <tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "        <tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "        <tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "We'll start by defining the structure of our network:\n",
    "<img src=\"XOR-nn.png\" width=\"60%\">\n",
    "\n",
    "- The first layer (aka the input layer) has two inputs corresponding to $a$ and $b$.\n",
    "- The middle / hidden layer is composed of three neurons.\n",
    "- The final layer (aka the output layer) has two outputs. \n",
    "\n",
    "The output of the neural network is a vector of length 2 where the first entry is the probability of the result being 0 and the second entry is the probability of the result being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward\n",
    "\n",
    "It's called a **Feed-Forward Neural Net** because we **feed the input forward** through the network starting at the input layer until the output.\n",
    "\n",
    "Here's how we implement the feed forward algorithm.\n",
    "\n",
    "$$\n",
    "Z_1 = X_1 \\cdot W_1 \\\\\n",
    "X_2 = \\text{ReLU}(Z_1) \\\\\n",
    "Z_2 = X_2 \\cdot W_2 \\\\\n",
    "\\hat{Y} = \\text{Softmax}(Z_2)\n",
    "$$\n",
    "\n",
    "Note: $\\large \\cdot$ represents matrix multiplication.\n",
    "\n",
    "To start we'll get some notation out of the way:\n",
    "1. **X1** is the input. \n",
    "    - It can either be a single instance i.e. \\[0, 0\\] (1 x 2) or a batch of instances \\[[0,0],[0,1],[1,0]] (3 x 2)\n",
    "1. **W1** is the first weight matrix with a shape of (2 x 3)\n",
    "2. **W2** is the second weight matrix with a shape of (3 x 2)\n",
    "\n",
    "- We forward our input through the first layer and get out $Z_1$. \n",
    "- We then apply a ReLU activation function on $Z_1$ and get $X_2$. \n",
    "- We then forward $X_2$ through the second layer and get $Z_2$.\n",
    "- Finally we apply softmax on $Z_2$ to get a vector of probabilities, $\\hat{Y}$, for each class (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "y:\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "\n",
      "y is \"one-hotted\". \n",
      "\n",
      "Each row in X corresponds to an output row in y. \n",
      "\n",
      "Since the first row of y has a 1 in \n",
      "the zeroth position, it means that \n",
      "the output is a 0.\n",
      "\n",
      "Since in the second row the 1 is in the 1st\n",
      "position, it means the output is a 1.\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# y is \"one-hotted\": Since the first row\n",
    "# has a 1 in the zeroth position, it means\n",
    "# that the output is a 0.\n",
    "# Since in the second row the 1 is in the 1st\n",
    "# position, it means the output is a 1.\n",
    "y = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
    "\n",
    "print('X:')\n",
    "print(X, '\\n')\n",
    "print('y:')\n",
    "print(y)\n",
    "\n",
    "print('''\n",
    "y is \"one-hotted\". \n",
    "\n",
    "Each row in X corresponds to an output row in y. \n",
    "\n",
    "Since the first row of y has a 1 in \n",
    "the zeroth position, it means that \n",
    "the output is a 0.\n",
    "\n",
    "Since in the second row the 1 is in the 1st\n",
    "position, it means the output is a 1.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the layers\n",
    "\n",
    "It's common practice to initialize the weights of each layer by drawing from a uniform distribution ranging from \n",
    "$$\n",
    "-\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}} \\to \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "\n",
    "also known as **Glorot uniform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[ 0.10694503  0.47145628  0.22514328]\n",
      " [ 0.09833413 -0.16726395  0.31963799]] \n",
      "\n",
      "W2\n",
      "[[-0.13673957  0.85833164]\n",
      " [ 1.01583421 -0.25536684]\n",
      " [ 0.63913754  0.0633056 ]] \n",
      "\n",
      "Preliminary predictions:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-30a5847ff6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preliminary predictions:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Ws' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "layer1 = nn.Layer(2, 3)\n",
    "layer2 = nn.Layer(3, 2)\n",
    "\n",
    "print('W1')\n",
    "print(layer1.weights, '\\n')\n",
    "print('W2')\n",
    "print(layer2.weights, '\\n')\n",
    "\n",
    "print('Preliminary predictions:')\n",
    "forward(Ws, X)[-1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the network is predicting everything to be a 0. \n",
    "\n",
    "So we need to learn right weights to give the correct output. This is where backpropagation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In backpropagation we learn what the right set of weights are in order to give the desired output.\n",
    "\n",
    "We assume we have a cost function (denoted $J$, in this case cross-entropy loss). We find the partial derivatives of the cost function with respect to each weight matrix. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_2} = \\frac{\\partial J}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial W_2} \\\\\n",
    "\\frac{\\partial J}{\\partial W_1} = \\frac{\\partial J}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial X_2} \\cdot \\frac{\\partial X_2}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
